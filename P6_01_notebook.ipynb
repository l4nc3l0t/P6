{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import ast\n","import plotly.express as px\n","import string\n","import nltk\n","\n","nltk.download(['punkt', 'stopwords', 'wordnet', 'omw-1.4'],\n","              '.env/lib/nltk_data')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, \\\n","                                            HashingVectorizer\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import adjusted_rand_score, confusion_matrix\n","from sklearn.utils import column_or_1d\n","\n","\n","class MyLabelEncoder(LabelEncoder):\n","\n","    def fit(self, y):\n","        y = column_or_1d(y, warn=True)\n","        self.classes_ = pd.Series(y).unique()\n","        return self\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["write_data = True\n","\n","# True : création d'un dossier Figures et Tableau\n","# dans lesquels seront créés les éléments qui serviront à la présentation\n","# et écriture des figures et tableaux dans ces dossier\n","#\n","# False : pas de création de dossier ni de figures ni de tableaux\n","\n","if write_data is True:\n","    try:\n","        os.mkdir(\"./Figures/\")\n","    except OSError as error:\n","        print(error)\n","    try:\n","        os.mkdir(\"./Tableaux/\")\n","    except OSError as error:\n","        print(error)\n","else:\n","    print(\"\"\"Visualisation uniquement dans le notebook\n","    pas de création de figures ni de tableaux\"\"\")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('./flipkart_com-ecommerce_sample_1050.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CategoryTree = data.product_category_tree.str.slice(\n","    start=2, stop=-2).str.split(' >> ', expand=True)\n","for i in CategoryTree.columns:\n","    CategoryTree.rename(columns={i: 'category_{}'.format(i)}, inplace=True)\n","CategoryTree.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CategoryTree.info()"]},{"cell_type":"markdown","metadata":{},"source":[" Les 3 premières hiérarchies de catégories sont bien remplies les suivantes à moitié"]},{"cell_type":"markdown","metadata":{},"source":[" nettoyage des spécifications produits et mise sous forme de dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# nettoyage\n","Specifications = data.product_specifications.str.replace(\n","    '\"product_specification\"=>', '',\n","    regex=True).str.replace('{\"key\"=>', '', regex=True).str.replace(\n","        ', \"value\"=>', ':',\n","        regex=True).str.replace('},', ',', regex=True).str.replace(\n","            '[', '', regex=True).str.replace(']}', '', regex=True)\n","Specifications.head(3)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def try_the_eval(row):\n","    try:\n","        ast.literal_eval(row)\n","    except:\n","        print('Found bad data: {0}'.format(row))\n","\n","\n","evalError = Specifications.apply(try_the_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SpecificationsClean = Specifications.str.replace('{\"value\"=>',\n","                                                 '\"unknown\":',\n","                                                 regex=True)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evalError = SpecificationsClean.apply(try_the_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SpecificationsClean = SpecificationsClean.str.replace('{nil}',\n","                                                      '{\"unknown\":\"unknown\"}',\n","                                                      regex=True).str.replace(\n","                                                          '}}',\n","                                                          '}',\n","                                                          regex=True)\n","SpecificationsClean.fillna('{\"unknown\":\"unknown\"}', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# mise sous forme de dictionnaire puis de dataframe\n","for i in SpecificationsClean.index:\n","    spec = ast.literal_eval(SpecificationsClean[i])\n","    if i == 0:\n","        ProdSpec = pd.DataFrame(columns=spec.keys())\n","        ProdSpec = ProdSpec.merge(pd.DataFrame(spec, index=[i]),\n","                                  on=ProdSpec.columns[ProdSpec.columns.isin(\n","                                      spec.keys())].to_list(),\n","                                  how='outer')\n","    else:\n","        ProdSpec = ProdSpec.merge(pd.DataFrame(spec, index=[i]),\n","                                  on=ProdSpec.columns[ProdSpec.columns.isin(\n","                                      spec.keys())].to_list(),\n","                                  how='outer')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ProdSpec.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# visualisation des colonnes comportant le plus de données\n","ProdSpec.isna().sum().sort_values().head(15)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# conservation des 7 colonnes contenant le plus de données\n","ProdSpecClean = ProdSpec[ProdSpec.isna().sum().sort_values().head(\n","    8).index.drop('unknown').to_list()]\n","ProdSpecClean.head()\n","#ProdSpecCleanFill = ProdSpecClean.fillna('unknown')\n","#ProdSpecCleanFill.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# utilisation des 3 premières branches de catégories\n","TextData = CategoryTree.iloc[:, :3].merge(\n","    ProdSpecClean, left_index=True,\n","    right_index=True).merge(data[['pid', 'description']],\n","                            left_index=True,\n","                            right_index=True).set_index('pid').reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# liste des identifiants produits et des fichiers d'image associés\n","ImgList = data[['pid', 'image']]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# nettoyage ponctuation, nombres\n","def clean_text(text):\n","    textNpunct = ''.join(\n","        [char for char in text if char not in string.punctuation])\n","    textNnum = ''.join([char for char in textNpunct if not char.isdigit()])\n","    textClean = textNnum\n","    return textClean\n","\n","\n","Descriptions = TextData.description.apply(lambda x: clean_text(x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tokenisation\n","Tokens = {}\n","for r in range(len(TextData)):\n","    Tokens[TextData.pid[r]] = nltk.word_tokenize(Descriptions.loc[r].lower())\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# nettoyage stopwords\n","def cleanStopW(List, stopW=[], addtolist=[], index=TextData.pid):\n","    for atl in addtolist:\n","        stopW.append(atl)\n","    ListClean = {}\n","    for r in range(len(index)):\n","        ListClean[index[r]] = [\n","            word for word in List[index[r]] if not word.isdigit()\n","            if word not in stopW\n","        ]\n","    return ListClean\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stopW = []\n","stopW = nltk.corpus.stopwords.words('english')\n","TokensStopW = cleanStopW(Tokens, stopW, string.ascii_lowercase)\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Visualisation des mots ayant le plus d'occurences dans tout le jeux de données"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def visuWordList(Words, listname='Tokens'):\n","    FullW = []\n","    for r in range(len(TextData)):\n","        for item in [*Words[TextData.pid[r]]]:\n","            FullW.append(item)\n","    FreqWFull = pd.DataFrame({\n","        listname: nltk.FreqDist(FullW).keys(),\n","        'Freq': nltk.FreqDist(FullW).values()\n","    })\n","    FreqWFull['Freq_%'] = round(FreqWFull.Freq * 100 / FreqWFull.Freq.sum(), 2)\n","    print(FreqWFull.sort_values(['Freq'], ascending=False).head(20))\n","\n","    fig = px.bar(FreqWFull.sort_values(['Freq'], ascending=False).head(50),\n","                 x=listname,\n","                 y='Freq',\n","                 width=900,\n","                 labels={\n","                     'Freq': \"Nb d'occurences\",\n","                 })\n","    return FullW, FreqWFull, fig\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FullTok, FreqTokFull, fig = visuWordList(TokensStopW)\n","fig.show(renderer='notebook')\n","if write_data is True:\n","    fig.write_image('./Figures/FreqTok50.pdf')\n","    fig.write_image('./Figures/FreqTok50.pdf')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Nous allons retirer les mots ayant plus de 400 occurences (> 0,8%)\n"," car on observe une certaine rupture à ce palier et ces mots concernent\n"," l'aspect commercial et non le produit lui même. Nous allons conserverons\n"," que les mots ayant au moins 3 occurences (>= 0.01%)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if write_data is True:\n","    FreqTokFull[FreqTokFull['Freq'] > 400].Tokens.to_latex(\n","        './Tableaux/Mots400+.tex', index=False)\n","FiltMots = FreqTokFull[(FreqTokFull['Freq'] > 400) |\n","                       (FreqTokFull['Freq'] < 3)].Tokens.to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TokensClean = cleanStopW(TokensStopW, stopW, FiltMots)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# lemmatisation\n","Lems = {}\n","for r in range(len(TextData)):\n","    Lems[TextData.pid[r]] = [\n","        nltk.WordNetLemmatizer().lemmatize(word)\n","        for word in TokensClean[TextData.pid[r]]\n","    ]"]},{"cell_type":"markdown","metadata":{},"source":[" Visualisation des lemmes ayant le plus d'occurences dans tout le jeux de données"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FullLem, FreqLemFull, fig = visuWordList(Lems, 'Lemmes')\n","fig.show(renderer='notebook')\n","if write_data is True:\n","    fig.write_image('./Figures/FreqLem50.pdf')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Nous allons supprimer le lemme ayant plus de 500 occurences (> 1.3%)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if write_data is True:\n","    FreqLemFull[FreqLemFull['Freq'] > 500].Lemmes.to_latex(\n","        './Tableaux/Lemmes1+.tex', index=False)\n","Lemmes1 = FreqLemFull[FreqLemFull['Freq'] > 500].sort_values(\n","    ['Freq'], ascending=False).Lemmes.to_list()\n","print(Lemmes1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["LemsClean = cleanStopW(Lems, stopW, FiltMots + Lemmes1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# racinisation (stemming)\n","Stems = {}\n","for r in range(len(TextData)):\n","    Stems[TextData.pid[r]] = [\n","        nltk.stem.PorterStemmer().stem(word)\n","        for word in TokensClean[TextData.pid[r]]\n","    ]"]},{"cell_type":"markdown","metadata":{},"source":[" Visualisation des racines ayant le plus d'occurences dans tout le jeux de données"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FullStem, FreqStemFull, fig = visuWordList(Stems, 'Racines')\n","fig.show(renderer='notebook')\n","if write_data is True:\n","    fig.write_image('./Figures/FreqStem50.pdf')"]},{"cell_type":"markdown","metadata":{},"source":[" Nous allons supprimer le stemme ayant plus de 500 occurences (> 1.3%)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if write_data is True:\n","    FreqStemFull[FreqStemFull['Freq'] > 500].Racines.to_latex(\n","        './Tableaux/Stemmes1+.tex', index=False)\n","Stemmes1 = FreqStemFull[FreqStemFull['Freq'] > 500].sort_values(\n","    ['Freq'], ascending=False).Racines.to_list()\n","print(Stemmes1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["StemsClean = cleanStopW(Stems, stopW, FiltMots + Stemmes1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Comparaison texte brute, tokens, lemmatisation, racinisation\n","CompareTxt = pd.DataFrame({\n","    'Modification':\n","    ['Texte brut', 'Tokenisation', 'Lemmatisation', 'Racinisation'],\n","    'Contenu': [\n","        TextData.description[0],\n","        ' '.join(tok for tok in TokensClean[TextData.pid[0]]),\n","        ' '.join(lem for lem in LemsClean[TextData.pid[0]]),\n","        ' '.join(stem for stem in StemsClean[TextData.pid[0]])\n","    ]\n","})\n","if write_data is True:\n","    CompareTxt.to_latex('./Tableaux/CompareTxt.tex', index=False)\n","CompareTxt\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clustering(corpora,\n","               vectorizer=[TfidfVectorizer()],\n","               TokenType=None,\n","               perplexity=[10, 20, 30, 40, 50],\n","               n_componentsPCA=0.98):\n","    Labels = {}\n","\n","    color_discrete_map = {}\n","    category_orders = TextData.category_0.sort_values().unique()\n","    for cat, col in zip(TextData.category_0.unique(),\n","                        px.colors.qualitative.D3[0:6]):\n","        color_discrete_map[cat] = col\n","\n","    Scores = pd.DataFrame(columns=['Vectorizer', 'perplexityTSNE', 'ARI'])\n","    i = 0\n","\n","    for p in perplexity:\n","        for vec in vectorizer:\n","            if str(vec) == str(HashingVectorizer()):\n","                cv = CountVectorizer()\n","                vec = HashingVectorizer(\n","                    n_features=cv.fit_transform(corpora).get_shape()[1])\n","                vectorizedTok = vec.fit_transform(corpora)\n","                vectorizedTokDF = pd.DataFrame(vectorizedTok.toarray(),\n","                                               TextData.pid,\n","                                               cv.get_feature_names_out())\n","            else:\n","                vectorizedTok = vec.fit_transform(corpora)\n","                vectorizedTokDF = pd.DataFrame(vectorizedTok.toarray(),\n","                                               TextData.pid,\n","                                               vec.get_feature_names_out())\n","\n","            pca = PCA(n_components=n_componentsPCA, random_state=0)\n","            vecTokPCA = pca.fit_transform(vectorizedTokDF)\n","            print('Réduction de dimensions : {} vs {}'.format(\n","                pca.n_components_, pca.n_features_))\n","\n","            tsneVTok = TSNE(n_components=2,\n","                            perplexity=p,\n","                            learning_rate='auto',\n","                            random_state=0,\n","                            init='pca',\n","                            n_jobs=-1).fit_transform(vecTokPCA)\n","\n","            tsnefig = px.scatter(tsneVTok,\n","                                 x=0,\n","                                 y=1,\n","                                 color=TextData.category_0,\n","                                 color_discrete_map=color_discrete_map,\n","                                 category_orders={'color': category_orders},\n","                                 labels={'color': 'Catégories'},\n","                                 opacity=1,\n","                                 title='t-SNE{} {} {}'.format(\n","                                     p,\n","                                     str(vec).split('(')[0], TokenType))\n","            tsnefig.update_traces(marker_size=4)\n","            tsnefig.update_layout(legend={'itemsizing': 'constant'})\n","            tsnefig.show(renderer='notebook')\n","            if write_data is True:\n","                tsnefig.write_image('./Figures/tsne{}{}{}.pdf'.format(\n","                    p, TokenType,\n","                    str(vec).split('(')[0]))\n","\n","            vecKMeans = KMeans(n_clusters=7, random_state=0).fit(tsneVTok)\n","\n","            LabelsDF = pd.DataFrame({\n","                'Catégories réelles': TextData.category_0,\n","                'Labels KMeans': vecKMeans.labels_\n","            })\n","            labelsGroups = LabelsDF.groupby(\n","                ['Catégories réelles'])['Labels KMeans'].value_counts()\n","            LabelsClean = labelsGroups.groupby(\n","                level=0).max().sort_values().reset_index().join(\n","                    pd.Series(\n","                        labelsGroups.groupby(\n","                            level=1).max().sort_values().index.to_list(),\n","                        name='Label maj')).rename(\n","                            columns={\n","                                'Labels KMeans': 'Nb prod/label'\n","                            }).sort_values('Label maj').reset_index(drop=True)\n","            print(LabelsClean)\n","            #print(labelsGroups)\n","\n","            le = MyLabelEncoder()\n","            le.fit(LabelsClean['Catégories réelles'])\n","\n","            LabelsDF['Labels réels'] = le.transform(\n","                LabelsDF['Catégories réelles'])\n","            LabelsDF['Catégories KMeans'] = le.inverse_transform(\n","                LabelsDF['Labels KMeans'])\n","            LabelsDF.reindex(columns=[\n","                'Catégories réelles', 'Labels réels', 'Labels KMeans',\n","                'Catégories KMeans'\n","            ])\n","\n","            CM = confusion_matrix(LabelsDF['Catégories KMeans'],\n","                                  LabelsDF['Catégories réelles'])\n","            fig = px.imshow(\n","                CM,\n","                x=category_orders,\n","                y=category_orders,\n","                text_auto=True,\n","                color_continuous_scale='balance',\n","                title=\n","                'Matrice de confusion des labels prédits (x) et réels (y)<br>t-SNE{} {} {}'\n","                .format(p,\n","                        str(vec).split('(')[0], TokenType))\n","            fig.update_layout(plot_bgcolor='white')\n","            fig.update_coloraxes(showscale=False)\n","            fig.show(renderer='notebook')\n","            if write_data is True:\n","                fig.write_image('./Figures/HeatmapLabels{}{}{}.pdf'.format(\n","                    p, TokenType,\n","                    str(vec).split('(')[0]))\n","\n","            ARI = adjusted_rand_score(LabelsDF['Labels réels'],\n","                                      LabelsDF['Labels KMeans'])\n","\n","            Scores.loc[i, 'Vectorizer'] = str(vec).split('(')[0]\n","            Scores.loc[i, 'perplexityTSNE'] = str(p)\n","            Scores.loc[i, 'ARI'] = ARI\n","            Scores.loc[i, 'TokenType'] = TokenType\n","            i += 1\n","\n","            kmeansfig = px.scatter(tsneVTok,\n","                                   x=0,\n","                                   y=1,\n","                                   title='KMeans t-SNE{} {} {}'.format(\n","                                       p,\n","                                       str(vec).split('(')[0], TokenType),\n","                                   color=LabelsDF['Catégories KMeans'],\n","                                   color_discrete_map=color_discrete_map,\n","                                   category_orders={'color': category_orders},\n","                                   labels={'color': 'Catégories'})\n","            kmeansfig.update_traces(marker_size=4)\n","            kmeansfig.update_layout(legend={'itemsizing': 'constant'})\n","            kmeansfig.show(renderer='notebook')\n","            if write_data is True:\n","                kmeansfig.write_image('./Figures/kmean{}{}{}.pdf'.format(\n","                    p, TokenType,\n","                    str(vec).split('(')[0]))\n","\n","            print('ARI :{}'.format(ARI))\n","\n","    return Scores\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["corporaTok = []\n","for r in range(len(TextData)):\n","    corporaTok.append(' '.join(tok for tok in TokensClean[TextData.pid[r]]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TokScores = clustering(\n","    corporaTok, [TfidfVectorizer(),\n","                 CountVectorizer(),\n","                 HashingVectorizer()], 'Tokens')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(TokScores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["corporaLem = []\n","for r in range(len(TextData)):\n","    corporaLem.append(' '.join(lem for lem in LemsClean[TextData.pid[r]]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["LemScores = clustering(\n","    corporaLem, [TfidfVectorizer(),\n","                 CountVectorizer(),\n","                 HashingVectorizer()], 'Lemmes')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(LemScores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["corporaStem = []\n","for r in range(len(TextData)):\n","    corporaStem.append(' '.join(stem for stem in StemsClean[TextData.pid[r]]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["StemScores = clustering(\n","    corporaStem, [TfidfVectorizer(),\n","                  CountVectorizer(),\n","                  HashingVectorizer()], 'Racines')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(StemScores)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ScoresFull = TokScores.merge(LemScores,\n","                             on=TokScores.columns.to_list(),\n","                             how='outer').merge(StemScores,\n","                                                on=TokScores.columns.to_list(),\n","                                                how='outer')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.bar(ScoresFull,\n","             x='perplexityTSNE',\n","             y='ARI',\n","             color='Vectorizer',\n","             facet_col='TokenType',\n","             barmode='group')\n","fig.show(renderer='notebook')\n","if write_data is True:\n","    fig.write_image('./Figures/CompareScores.pdf')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}